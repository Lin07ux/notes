> 转摘：[字节一面，被问到两个经典问题！](https://mp.weixin.qq.com/s/sK2caRVxmkXInKcxtDsTVg)

## 一、基础介绍

TCP 连接建立的时候需要三次握手，关闭的时候则需要四次挥手，这是因为 TCP 是全双工模式，在一方关闭的时候，另一方可能还有数据需要继续发送，就需要分成两阶段分别关闭 TCP 连接的两端。

TCP 的四次挥手的状态变化如下图所示：

![](https://cnd.qiniu.lin07ux.cn/markdown/1671534044)

主动关闭方在主动调用`close()`系统调用后，依次进入`FIN_WAIT_1`、`FIN_WAIT_2`、`TIME_WAIT`和`CLOSE`状态，而被关闭方在调用`read()`系统调用得到`EOF`响应后，会依次进入`CLOSE_WAIT`、`LAST_ACK`、`CLOSE`状态。

### 1.2 TIME_WAIT

`TIME_WAIT`状态是主动关闭连接方才会出现的状态，而且该状态会持续 2MSL 时间才会进入到`CLOSE`状态。在 Linux 上，2MSL 的时长是 60 秒，也就是说**主动关闭连接方停留在`TIME_WAIT`的时间为固定的 60 秒**。

进入到`TIME_WAIT`状态说明自身已经主动关闭连接了，而且也接受到对方的关闭连接的`FIN`报文了，那么为什么不能直接进入到`CLOSE`状态关闭连接而要停留在`TIME_WAIT`状态呢？主要有两个原因：

* **保证*被动关闭连接*的一方能被正确的关闭**：四次挥手中，被动关闭连接方发送过来的`FIN`报文是需要主动关闭连接方回复`ACK`报文的，而该`ACK`报文也有可能会丢失。被动关闭连接方在一定时间后没能收到`ACK`响应时，会尝试继续发送`FIN`报文。如果这时主动关闭连接方处于`CLOSE`状态，就会响应`RST`报文而不是`ACK`报文。所以主动关闭连接方需要处于`TIME_WAIT`状态，而不能直接进入到`CLOSE`状态。
* **防止历史连接中的数据被后面相同四元组的连接错误的接受**。TCP 报文会因为路由器异常而“迷路”，在其迷途期间，TCP 发送端可能会因为确认超时而重发这个报文。而迷途的报文在路由器修复后也会被送到最终目的地，这个原来的迷途报文就被成为 lost duplicate。在关闭一个 TCP 连接后，马上又重新建立起一个相同的 IP 地址和端口之间的 TCP 连接，后一个连接被称为前一个连接的化身。此时就可能会出现这种情况：前一个连接的迷途重复报文在前一个连接终止后出现，从而被误解为从属于新的连接。为了避免这种情况，`TIME_WAIT`状态需要持续 2MSL 时间，这样就可以保证当成功建立一个 TCP 连接的时候，来自前一个连接的重复报文已经在网络中消逝了。

### 1.3 CLOSE_WAIT

`CLOSE_WAIT`状态是被动关闭方会经历的状态，而且如果被动关闭方没有调用`close()`来关闭连接，那么就一直不会发出 FIN 报文，从而就一直处于`CLOSE_WAIT`状态。

## 二、TIME_WAIT 状态过多

### 2.1 TIME_WAIT 状态的 TCP 连接过多的危害

过多的 TIME_WAIT 状态主要的危害主要就是占用各种系统资源：**文件描述符、内存资源、CPU 资源、端口资源**等。系统的端口资源也是有限的，一般可以开启的端口为 32768～61000，也可以通过`net.ipv4.ip_local_port_range`参数来修改 Linux 系统的端口范围。

客户端和服务端 TIME_WAIT 过多造成的影响是不同的：

* 客户端：受限于 TCP 连接的四元组因素，当客户端的 TIME_WAIT 状态过多，占满了所有端口资源，那么就无法对`目的 IP + 目的 Port`都相同的服务端发起连接了，不过还能对其他的服务端发起连接。

* 服务端：服务端的 TIME_WAIT 过多并不会导致端口资源受限，因为服务端只监听一个端口，而客户端的 IP 和 Port 大都不同，所以可以理论上可以建立很多的连接。只是大量的 TIME_WAIT 状态的连接会占用各种系统资源，影响系统性能和稳定性。

### 2.2 优化 TIME_WAIT 状态

TIME_WAIT 状态的持续时间有一点长，显得很不友好，但是他被设计来就是用来避免发生各种意外情况的。《Unix 网络编程》一书中有说：**TIME_WAIT 是我们的朋友，它是有助于我们的，不要试图避免这个状态，而是应该弄清楚它。如果服务端要避免过多的 TIME_WAIT 状态的 TCP 连接，就永远不要主动断开连接，让客户端去断开，由分布在各处的客户端去承受 TIME_WAIT。**

不过，因为各种原因，想要在 Linux 中优化 TIME_WAIT 状态，有如下三种方式：

* 打开`net.ipv4.tcp_tw_reuse`和`net.ipv4.tcp_timestamps`选项；
* `net.ipv4.tcp_max_tw_buckets`
* 程序中使用`SO_LINGER`，强制使用`RST`关闭。

**net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps**

开启`net.ipv4.tcp_tw_reuse`可以复用处于 TIME_WAIT 的 socket 为新的连接所用。需要注意的是，*该选项功能只能用在客户端（连接发起方）*。

开启该功能后，在调用`connect()`的时候，内核会随机找一个 TIME_WAIT 状态超过 1 分钟的连接给新的连接复用。

另外，使用这个选项的前提是要打开对 TCP 时间戳（默认开启）的支持：

```
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_timestamps = 1
```

这个时间戳的字段在 TCP 头部的选项部分中，由一共 8 个字节表示时间戳，其中前 4 个字节用来保存发送该数据包的时间，后 4 个字节用来保存最近一次接收对方发送达到数据的时间。

由于引入了时间戳，可以使得重复的数据包因时间戳过期被自然丢弃，因此 TIME_WAIT 状态的连接才能被复用。

**net.ipv4.tcp_max_tw_buckets**

Linux 系统中这个值默认为 18000。当系统中处于 TIME_WAIT 状态的连接数量超过这个值时，系统就会将后面的 TIME_WAIT 状态的连接进行状态重置：

```
net.ipv4.tcp_max_tw_buckets = 18000
```

**程序中使用 SO_LINGER**

可以通过设置 socket 选项来设置调用`close()`关闭连接的行为。

`so_linger`有两个配置项：`l_onoff`和`l_linger`。如果`l_onoff`为非 0 且`l_linger`的值为 0，那么调用`close()`的时候内核会立即发送一个 RST 标志给对端。此时 TCP 连接将跳过四次挥手，直接关闭，自然也就跳过了 TIME_WAIT 状态。

这虽然为跨越 TIME_WAIT 状态提供了一个可能，但这是一个非常危险的行为，不值得提倡。

### 2.3 Web 服务器出现大量的 TIME_WAIT 状态的原因

如果 Web 服务端出现大量的 TIME_WAIT 状态的 TCP 连接，说明服务端主动断开了很多 TCP 连接。在以下场景下，服务端会主动断开 TCP 连接：

1. HTTP 没有使用长连接
2. HTTP 长连接超时
3. HTTP 长连接的请求数量达到上限

**HTTP 没有使用长连接**

从 HTTP/1.1 开始，默认就使用长连接了，而且现在主流使用的就是 HTTP/1.1 和 HTTP/2.0 了，所以一般 Web 服务都是使用长连接的。但是在 HTTP 连接建立后，客户端和服务端任意一方的 HTTP Header 中设置了`Connection: close`信息，就会导致该连接被关闭。

在 RFC 文档中，并没有明确规定由服务端还是客户端主动关闭连接。不过大多数的 Web 服务器的实现中，不管是哪一方禁用了 HTTP Keep-Alive **都是由服务器端主动关闭连接**。此时服务器端上就会出现 TIME_WAIT 状态的连接。

* 客户端禁用了 HTTP Keep-Alive，服务端开启 HTTP Keep-Alive，服务端在发完 HTTP 响应后，就会主动关闭连接。

    HTTP 是请求-响应模型，发起方一直是客户端。HTTP Keep-Alive 的初衷是为客户端后续的请求重用连接。如果在某次 HTTP 请求中，Header 中定义了`Connection: close`信息，那不再重用这个连接的时机就只有在服务端了。所以在 HTTP 请求-响应这个周期的末端关闭连接是合理的。

* 客户端开启了 HTTP Keep-Alive，服务端关闭了 HTTP Keep-Alive，服务端在发完 HTTP 响应后，就会主动关闭连接。

    在服务端主动关闭连接的情况下，主要调用一次`close()`就可以释放连接，剩下的工作由内核 TCP 栈直接进行了处理，整个过程只有一次系统调用；如果要求客户端关闭连接，则服务端在写完最后一个响应之后需要把这个 socket 重新放入到 readable 队列，再次调用`select/epoll`去等待事件，然后调用一次`read()`才能知道连接已经被关闭，这就有两次系统调用了，而且多一次用户端程序被激活执行，socket 保持时间也会更长。
    
因此，**当服务端出现大量的 TIME_WAIT 状态的 TCP 连接的时候，可以排查下是否客户端和服务端都开启了 HTTP Keep-Alive**。因为任意一方没有开启 HTTP Keep-Alive，都会导致服务端在处理完一个 HTTP 请求后，就主动关闭连接。此时服务端上就会出现大量的 TIME_WAIT 状态的 TCP 连接。

**HTTP 长连接超时**

HTTP 长连接的特点时，只要两端都没有明确提出断开连接，则会保持 TCP 连接状态一段时间。为了避免资源浪费的情况，Web 服务器一般会提供一个参数来指定 HTTP 长连接的超时时间，比如 Nginx 提供的`keepalive_timeout`参数。

假设设置了 HTTP 长连接的超时时间是 60 秒，那么 Nginx 会为为连接启动一个定时器，如果客户端在完成一次 HTTP 请求后的 60 秒内没有再发起新的请求，定时器的时间一到 Nginx 就会关闭该连接，此时服务器端上就会出现 TIME_WAIT 状态的 TCP 连接了。

当服务端出现大量的 TIME_WAIT 状态的连接时，如果现象是有大量的客户端建立完 TCP 连接后很长一段时间没有发送数据，那么大概率是因为 HTTP 长连接超时，导致服务端主动关闭连接。

此时可以往网络问题的方向排查，比如是否因为网络问题导致客户端发送的数据一直没有被服务端接收到，以至于 HTTP 长连接超时。

**HTTP 长连接的请求数量达到上限**

Web 服务端通常会有个参数来定义一条 HTTP 长连接上最大能处理的请求数量，当超过最大限制时，就会主动关闭连接。

比如 Nginx 的`keepalive_requests`这个参数，是指一个 HTTP 长连接建立之后能通过其接收并处理的客户端请求的数量。如果在这个长连接上处理的请求的数量达到该参数的值时，Nginx 就会主动关闭这个长连接，从而在服务端出现 TIME_WAIT 状态的 TCP 连接。

Nginx 中`keepalive_requests`的默认值为 100，意味着每个 HTTP 长连接最多只能跑 100 次请求，在 QPS 不是很高的时候，100 是凑合够用的。但是对于一些 QPS 很高的场景（10000 QPS，甚至 30000 QPS），100 的值就不够用了，会导致 Nginx 很频繁的关闭连接，使得服务端出现大量的 TIME_WAIT 状态的 TCP 链接。

这个场景下，解决方式很简单，就是调到请求数量限制的上限值。

## 三、CLOSE_WAIT 状态过多

### 3.1 Web 服务端出现过多的 CLOSE_WAIT 状态的 TCP 连接

**当服务端出现大量 CLOSE_WAIT 状态的连接的时候，说明服务端的程序一直没有调用`close()`来关闭连接。**

一个普通 TCP 服务端的流程为：

1. 创建服务端 socket，bind 绑定端口、listen 监听端口；
2. 将服务端 socket 注册到 epoll；
3. epoll_wait 等待连接到来，并用 accept 获取已连接的 socket；
4. 将已连接的 socket 注册到 epoll
5. epoll_wait 等待读事件发生
6. 对方关闭连接时，我方调用 close。

从这个流程中可以看出，服务端没有调用`close()`函数的原因可能有：

* 第二步没有做，没有将服务端 socket 注册到 epoll，这样有新连接到来时，服务端没办法感知这个时间，也就无法获取到已连接的 socket，服务端自然就没有机会对 socket 调用`close()`函数了。

    这种情况发生的概率比较小，属于明显的代码逻辑 bug。
    
* 第 3 步没有做，有新连接到来时没有调用 accept 获取该连接的 socket，导致当有大量客户端主动断开连接服务端没有机会对这些 socket 调用`close()`函数。

    发生这种情况可能是因为服务端在执行 accept 之前，代码卡在某一个逻辑或提前抛出了异常。
    
* 第 4 步没有做，通过 accept 获取已连接的 socket 后，没有将其注册到 epoll，导致后续收到 FIN 报文的时候，服务端没办法感知这个事件，自然服务端就没有机会调用 close 函数了。

    发生这种情况可能是因为服务端在将已连接的的 socket 注册到 epoll 之前，代码卡在某一个逻辑或者提前抛出了异常。可以参考下别人解决 CLOSE_WAIT 状态问题的实践文章：[一次 Netty 代码不健壮导致的大量 CLOSE_WAIT 连接原因分析](https://mp.weixin.qq.com/s?__biz=MzU3Njk0MTc3Ng==&mid=2247486020&idx=1&sn=f7cf41aec28e2e10a46228a64b1c0a5c&scene=21#wechat_redirect)。

* 第 6 步没有做，当发现客户端关闭连接后，服务端并没有执行 close 函数。

    这可能是因为代码漏处理，或者是在执行 close 函数之前，代码卡在某一个逻辑中，比如发生死锁等。
    
可以发现，当服务端出现大量 CLOSE_WAIT 状态的连接的时候，通常都是代码的问题。这就需要针对具体的代码一步步的进行排查和定位，主要分析方向就是找服务端为什么没有调用`close()`。

## 四、挥手丢失问题

### 4.1 第二次挥手丢失了会发生什么

当服务端收到客户端的第一次挥手报文后，就会先回复一个 ACK 确认报文，然后服务端的连接就进入到`CLOSE_WAIT`状态。

由于 ACK 报文是不会重传的，所以如果服务端的第二次挥手的这个 ACK 报文丢失了，客户端就会因为没有接到对应的确认接收信息而触发超时重传机制，重新发送第一次挥手的 FIN 报文，直到收到服务端的第二次挥手 ACK 报文，或者达到最大的重试次数。

例如，假设`tcp_orphan_retries`参数的值为 2，当第二次挥手报文一直丢失时，会发生如下图所示的过程：

![](https://cnd.qiniu.lin07ux.cn/markdown/1680419380)


具体过程为：

由于`tcp_orphan_retries = 2`，当客户端超时重试 2 次 FIN 报文后，达到最大的重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍）。如果还是没能收到服务端的第二次挥手 ACK 报文，那么客户端就会立即断开连接。

### 4.2 第三次挥手丢失了会发生什么

当服务端（被动关闭方）收到客户端（主动关闭方）的 FIN 报文后，内核会主动回复 ACK，同时将连接设置为`CLOSE_WAIT`状态，表示等待应用进程关闭连接。内核是没有权利替代应用进程来关闭连接的，必须有应用进程主动调用`close`等函数来触发服务端发送 FIN 报文，因为此时应用程序可能还有数据需要发给客户端。

服务端处于`CLOSE_WAIT`状态时，调用了`close`函数，内核就会发出 FIN 报文，也就是第三次挥手。同时连接进入`LAST_ACK`状态，等待客户端返回 ACK 来确认连接关闭。

如果这次挥手报文丢失了，那么服务端就因为超时未接收到 ACK 报文而触发重传，重传次数仍然由`tcp_orphan_retries`参数来控制，因为这也是一个 FIN 报文。这与客户端重发第一次挥手的 FIN 报文的重传次数控制方式是一样的。

例如，假设`tcp_orphan_retries = 3`，当第三次挥手一直丢失时，发生的过程如下图所示：

![](https://cnd.qiniu.lin07ux.cn/markdown/1680420227)

具体过程为：

由于`tcp_orpahn_retries = 3`，当服务端重传第三次挥手报文的次数达到 3 次后，因为达到重传的最大次数，于是会等待一段时间（时间为上次超时时间的 2 倍），在此期间如果还是没能收到客户端的第四次挥手 ACK 报文，那么服务端就会直接断开连接。

### 4.3 第四次挥手丢失了会发生什么

当客户端收到服务端的第三次挥手 FIN 报文后，就会回复 ACK 报文，也就是第四次挥手报文，同时客户端连接进入到`TIME_WAIT`状态。而服务端（被动关闭方）在没有收到第四次挥手 ACK 报文前，还是处于`LAST_ACK`状态。

> 在 Linux 系统中，`TIME_WAIT`状态会持续 2MSL 后才会进入连接关闭状态。

如果第四次挥手的 ACK 报文没有到达服务端，服务端就会因为超时触发重传 FIN 报文，和前面介绍的过程相同。

例如，假设`tcp_orphan_retries = 2`，当第四次挥手的报文一直丢失时，发生的过程如下：

![](https://cnd.qiniu.lin07ux.cn/markdown/1680420568)

具体过程为：

* 由于`tcp_orphan_retries = 2`，当服务端重传第三次挥手 FIN 报文达到 2 次时，由于达到最大重传次数，于是服务端会再等待一段时间（时间为上一次超时时间的 2 倍）。如果在此期间还是没能收到客户端的第四次挥手 ACK 报文，那么服务端就会断开连接。
* 客户端你在收到第三次挥手 FIN 报文后，就会进入`TIME_WAIT`状态，并开启时长为 2MSL 的定时器。如果在此期间再次收到第三次挥手 FIN 报文，就会再次回复 ACK 报文，并重置该定时器。当等到 2MSL 时长后，客户端就会断开连接。

## 五、其他问题

### 5.1 主动关闭方的 FIN_WAIT2 状态会持续多久

当客户端收到第二次挥手 ACK 报文后，客户端就会处于`FIN_WAIT2`状态。对于由`close`函数关闭的连接，由于无法再进行收发数据，所以`FIN_WAIT2`状态不需要持续太久，由`tcp_fin_timeout`参数控制这个状态下连接的持续时长，默认为 60 秒。这意味着对于调用`close`关闭的连接，如果在 60 秒后还没有收到第三次挥手的 FIN 报文，客户端（主动关闭方）的连接就会直接关闭，如下图所示：

![](https://cnd.qiniu.lin07ux.cn/markdown/1680419641)

但是注意：如果主动关闭方使用`shutdown`函数关闭连接，指定了只关闭发送方向，而接收方向没有关闭，那么意味着主动关闭方是可以继续接收数据的。此时如果主动关闭方一直没有收到第三次挥手的 FIN 报文，那么主动关闭方的连接将会一直处于`FIN_WAIT2`状态（`tcp_fin_timeout`参数无法控制`shutdown`关闭的连接）。如下图所示：

![](https://cnd.qiniu.lin07ux.cn/markdown/1680419900)

### 5.2 TIME_WAIT 状态等待的时长为什么是 2MSL

MSL(Maximum Segment Lifetime)，报文最大生存时长，是任何报文在网络上存在的最长时间，超过这个时间的报文将会被丢弃。

因为 TCP 报文是基于 IP 协议的，而 IP 协议头中有一个 TTL 字段，表示 IP 数据可以经过的最大路由数。每经过一个处理的路由器此值就会减 1。当此值为 0 时该 IP 数据报将被丢弃，同时发送 ICMP 报文通知源主机。

MSL 与 TTL 的区别在于：MSL 的单位是时间，而 TTL 则是经过路由跳数。所以 **MSL 应该要大于等于 TTL 缩减为 0 的时间**，以确保报文已自然消亡。

**TTL 的值一般是 64，而 Linux 将 MSL 设置为 30 秒**。这意味着 Linux 认为数据报经过 64 个路由器的时间不会超过 30 秒，如果超过了就认为该报文已经消失在网络中了。

`TIME_WAIT`状态等待的时长为 2MSL，比较合理的解释是：发送方的数据包被接收方接收至少需要 1MSL，当接收方处理后又向发送方返回响应，该响应达到发送方也至少需要 1MSL，所以一来一回就需要等待 2MSL 的时间。

比如，如果被动关闭方没有接收到断开连接的最后的 ACK 报文就会触发超时重发 FIN 报文，另一方收到 FIN 报文后会回复 ACK 给被动关闭方，这一来一回就需要 2MSL 了。

可以看到，**2MSL 时长**其实是相当于**至少允许报文丢失一次**。比如，若 ACK 在一个 MSL 内丢失，这样被动方重发的 FIN 会在第 2 个 MSL 内达到，`TIME_WAIT`状态的连接可以继续处理。

而为什么`TIME_WAIT`等待时长不是 4MSL 或 8MSL 呢？这其实是一个概率的问题，一般的网络状态足够好了，不需要过长的等待时间来避免报文丢失的问题。如果丢包率达到百分之一，那么连续两次丢包的概率就只有万分之一，这个概率已经足够小了，忽略它比解决它更具有性价比。

2MSL 的时长是从客户端为接收到的 FIN 报文回复 ACK 报文开始计时的。如果在次时间内再次收到服务端重发的 FIN 报文，那么 2MSL 时间将重新开始计时。

在 Linux 系统中，MSL 为 30 秒，所以`TIME_WAIT`等待时长就是 60 秒。MSL 时长的定义在 Linux 内核代码中南通过`TCP_TIMEWAIT_LEN`定义：

```c
#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT 
                                    state, about 60 seconds  */
```

如果要修改`TIME_WAIT`等待的时间，就只能修改 Linux 内核代码并重新编译。

### 5.3 为什么需要 TIME_WAIT 状态

TCP 连接的主动关闭方才有`TIME_WAIT`状态，其存在的原因主要有两个：

* 防止历史连接中的数据被后面相同四元组的连接错误接收；
* 保证「被动关闭连接」的一方能被正确的关闭。

**原因一：防止历史连接中的数据被后面相同四元组的连接错误的接收**

TCP 连接建立的时候，会通过随机的初始化序列号来尽量避免历史连接中的数据被错误接收的问题，但是由于初始化序列化并不是无线递增的，会发生回绕为初始值的情况，所以还是会存在一定的概率发生新连接接收了历史连接数据的问题。

假设`TIME_WAIT`没有等待时间或者等待时间过短，被延迟的数据包抵达后会发生如下图所示的影响：

![](https://cnd.qiniu.lin07ux.cn/markdown/1680421956)

也就是：

* 服务端在关闭连接之前发送的`SEQ = 301`的报文被网络延迟了。
* 然后客户端以相同的四元组与服务器重新建立了新连接，而前面被延迟的`SEQ = 301`的报文这时抵达了客户端，而且该数据报文的序列号刚好在客户端接收窗口内，那么客户端就会正常的接收这个数据报文，但这个数据报文是上一个连接残留下来的，这样就产生数据错乱等问题。

为了防止历史连接中的数据被后面相同四元组的连接错误的接收，TCP 设计了`TIME_WAIT`状态，并持续 2MSL 时长，这个时间足够让两个方向的数据报都被丢弃，使得原来连接的数据报文在网络中都自然消失，从而确保再出现的数据报文一定都是新建立的连接所产生的。

**原因二：保证「被动关闭连接」的一方能被正确的关闭**

在 RFC 793 中指出`TIME_WAIT`状态持续 2MSL 的另一个重要作用是：

> TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgement of its connection termination request.

也就是说，`TIME_WAIT`的作用是等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而使其正常关闭。

如果客户端（主动关闭方）最后一次 ACK 报文（第四次挥手）在网络中丢失了，那么按照 TCP 可靠性原则，服务端（被动关闭方）会重发 FIN 报文。假设客户端没有`TIME_WAIT`状态，而是在发送完最后一个 ACK 报文就直接进入`CLOSE`状态。当这个 ACK 报文丢失时，服务端重传的 FIN 报文就无法被正常接收和处理，客户端会直接回复 RST 报文来让服务端关闭连接：

![](https://cnd.qiniu.lin07ux.cn/markdown/1680422501)

RST 报文可以被解释为一个错误（Connection reset by peer），这对于一个可靠的协议来说不是一个优雅的终止方式。

为了防止出现这种情况，客户端必须等待足够长的时间，确保服务端能收到 ACK。如果服务端没有收到 ACK 就会触发重传机制重新发送一个 FIN，这样一来一回就刚好是 2MSL 的时间。

![](https://cnd.qiniu.lin07ux.cn/markdown/1680422606)
