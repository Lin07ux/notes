> 转摘：[面试官让我讲讲分布式系统容错架构，结果。。。](https://mp.weixin.qq.com/s/CH91eUsfVv6lqTVippFocA)

这篇文章使用非常浅显易懂的语言，讲述大规模分布式系统的容错架构设计。

### 1. TB 级数据存放在一台机器上

假如数据库中有一个很大的表，里面的数据有几十亿，甚至上百亿。其对应的存储空间可能有几十 TB，甚至上百个 TB。

![](https://cnd.qiniu.lin07ux.cn/markdown/1666166967)

此时是很危险的，因为如果用 MYSQL 之类的数据库，单独数据库服务器上的磁盘可能都不够放着一个表的数据了。

### 2. 分布式存储的概念

对于数据库来说，当数据集非常大的时候，就需要采取分库分表、多机器共同存储的方式来支撑了。

而对于上面假设的超大的数据集，放在一台机器上肯定是有很大的问题的，自然也需要考虑分开存放在多个机器中。

假设数据总共有 20TB，那么准备 20 台机器，这样每台机器上就只需要存放 1TB 的数据了，这样就能很轻松的支撑得住了。

所以说，把一个超大的数据集拆分成多片，放到多台机器上去，这就是所谓的**分布式存储**。

![](https://cnd.qiniu.lin07ux.cn/markdown/1666167445)

### 3. 分布式存储系统的概念

分布式存储系统，当然就是负责把一个超大数据集拆分成多块，然后放到多台机器上来存储，并且统一管理这些分散在多台机器上的数据的一套系统。

比如说经典的 Hadoop 就是这类系统，fastdfs 也是类似的。

如果把脑洞打开，从思想本质共通的层面触发，就会发现，其实 ElasticSearch、Redis Cluster 等等系统，本质都是如此。

这些都是基于分布式的系统架构，把超大数据拆分成多片，存放在多台机器上。

从分布式系统架构层面出发，不拘泥于任何一种技术，可以姑且设定，这套分布式存储系统有两种进程：

* 一个进程是 Master 节点，就在一台机器上，负责统一管控分散在多台机器上的数据；
* 另外一批进程叫做 Slave 节点，负责管理那台机器上的数据，跟 Master 节点进行通信。

结构类似如下：

![](https://cnd.qiniu.lin07ux.cn/markdown/1666167889)

### 4. 某台机器宕机了

从一台机器变成 20 台机器，那么出问题的概率也相应的增大了。如果其中一台机器宕机了怎么办呢？这会导致本来完成的一份 20TB 的数据只剩下 19TB 了，数据不完整了。

当然不能允许这种情况的发生，这个时候就必须做一个数据副本的策略。

比如说，可以给每台机器上的那 1TB 数据做 2 个副本的冗余，放在别的机器上。一旦某一台机器宕机，就可以使用其他机器上的副本来保证数据不丢失了。而存放冗余副本的机器，可以不另外增加机器，而就用存储每片数据的 Slave 节点服务器。

此时的架构设计图如下：

![](https://cnd.qiniu.lin07ux.cn/markdown/1666169211)

上图中的前脸色的“1TB数据01”，代表的是 20TB 数据集中的第一个 1TB 数据分片。

从图中可以看到，这个数据分片总共有 3 个副本，分别存放在三台机器中的浅蓝色的方块中。其他的数据分片也类似，都有三个副本。

此时如果有一台机器宕机了，比如就是下面图中红色的机器，此时必然会导致“1TB数据01”这个数据分片的其中一个数据副本丢失：

![](https://cnd.qiniu.lin07ux.cn/markdown/1666169439)

这个时候并不会造成数据丢失，因为“1TB数据01”还有另外两个副本放在其他的机器上的。如果有人要读取“1TB数据01”这个数据片，完全可以从另外两台机器上随便挑一个副本来读取就可以了。

### 5. Master 节点感知数据副本消失

现在有一个问题，比如说，有个客户端要读取“1TB数据01”这个数据分片，那么就需要与 Master 节点进行通讯，请求 Master 节点指出“1TB数据01”这个数据分片在哪台机器上。

如下图：

![](https://cnd.qiniu.lin07ux.cn/markdown/1666169631)

此时 Master 节点就要从“1TB数据03”的三个副本中选择一个出来，告诉客户端到对应的机器上去读取。

如果 Master 节点还不知道“1TB数据01”的副本3 所在的机器已经宕机了，而且回复客户端是从宕机的那个机器上读取，那么客户端就不能正常的请求到数据了。

怎么样才能让 Master 节点感知到“1TB数据01”的副本3 已经丢失了呢？

其实也很简单：每台机器上负责管理数据的 Slave 节点，每隔一定时间（比如 1 秒）就给 Master 节点发送一个心跳请求。

那么，一旦 Master 节点发现一段时间内（比如 30 秒内）一直没有收到某个 Slave 节点发过来的心跳，就会认为这个 Slave 节点所在机器宕机了，那台机器上的数据副本都丢失了。

然后 Master 节点就不会告诉客户端从这个宕机的 Slave 节点读取数据副本了。

![](https://cnd.qiniu.lin07ux.cn/markdown/1666169922)

此时，如果有客户端来请求读取“1TB数据01”数据分片，Master 节点就会让它从其他的两个节点中进行读取。

![](https://cnd.qiniu.lin07ux.cn/markdown/1666170054)

### 6. 复制副本保持足够多的副本数量

还有一个问题，就是“1TB数据01”这个数据分片此时只有两个副本了，而预设的是每个数据分片都要有 3 个副本。那么，如何给这个数据分片增加 1 个副本呢？

很简单：Master 节点一旦感知到某台机器宕机，就能知道某个数据分片的副本数量不足了，刺水就会生成一个副本复制的任务。挑选另外一台机器来从有副本的机器上去复制一个副本。

比如看下面的图，可以挑选第四台机器从第二台机器去复制一个副本：

![](https://cnd.qiniu.lin07ux.cn/markdown/1666170226)

那么这个复制任务如何通知到机器 4 呢？

其实也很简单，机器 4 不是每隔一段时间就会发送一次心跳请求吗？Master 节点可以在机器 4 发送过来的心跳请求中，把这个任务作为响应内容下发给机器 4。从而机器 4 就能开始从机器 2 中复制对应的数据分片的副本了。

过程如下图所示：

![](https://cnd.qiniu.lin07ux.cn/markdown/1666170450)

复制完成后，机器 4 上就会多了一个“1TB数据01”的副本3，从而使得“1TB数据01”的副本数量又变成 3 个了。

### 7. 删除多余副本

反过来，如果一段时间后，机器 3 突然恢复了，那么它上面的“1TB数据01”的副本3 就成了 “1TB数据01”的第四个副本了，有点多余了。

Master 节点在感知到机器 3 复活后，发现整个系统中，“1TB数据01”的副本数量过多，为了避免多余数据副本的存在，会生成一个删除副本的任务，在机器 3 的心跳请求的响应中下发给机器 3。

机器 3 在接到副本删除任务的时候，就会将自己机器上的多余的副本给删除掉，这样就使得副本数量保持为 3 个了。

如下图所示：

![](https://cnd.qiniu.lin07ux.cn/markdown/1666170732)

### 8. 总结

一个分布式存储系统完整的数据容错架构基本就如上述流程中分析设计的。

实际上，这种数据分片存储、多副本冗余、宕机感知、自动副本迁移、多余副本删除的机制，对于 Hadoop、ElasticSearch 等很多系统来说，都是类似的。

很多的分布式系统、中间件系统底层数据容错架构的思想都有相互联系、相互借鉴的情况。


