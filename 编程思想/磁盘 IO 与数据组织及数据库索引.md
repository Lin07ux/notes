> 转摘：[我也聊聊数据与磁盘IO](https://mp.weixin.qq.com/s/6MZcI7_kpZCphefvNQ22Rw)

说到磁盘，大多数人脑海里都能想到磁盘的物理模型——一个旋转轴上插着一个或多个磁性圆盘，盘每面上可能都有一个能来回移动的读写头，还有相关 CHS、LBA、DMA…… 一大堆词；结合物理模型和教科书上的金科玉律——“磁盘随机操作慢，顺序操作快“，但是在 OS、VM、各种库的层层包装下，在应用级别的 API 里我们如何利用这条定律，即”何为随机何为顺序？”

<img src="http://cnd.qiniu.lin07ux.cn/markdown/1555597761796.png" style="text-align:center"/>

## 一、从 Page Cache 说起

说到磁盘 IO 就不得不提到 page cache，很多人隐约知道 OS 会“缓存”磁盘的 IO 操作，但这种”缓存“是如何发生的？它有什么特性？会对系统/产品有什么影响？如何利用它的特性做设计？这就需要了解 page cache 的操作流程。

### 1.1 Read 时发生了什么

传统的 UNIX 思想信 “一切皆文件”，其后继者 Plan 9 更是将其发挥到了极致，在这种思想下，`read/write`就成了最重要的 syscall，所有通过 User/Kernel Mode 流通的磁盘、套接字、设备、管道、IPC…… 数据都需要通过`read/write syscall`，可见其为 UNIX 系统乃至今 Linux User Mode 的核心 API。

当应用调用`read`时除了 switch 到 kernel mode，kernel 需要取好数据并放到传入的 buf 地址处。该过程大致如下：

```
if page cache 中存在所需的数据块
    复制数据块到传入的 buf 地址处
    返回
else
    向 DMA 控制器发起读磁盘操作
    挂起该操作所在的进程，等待 DMA 操作完成
    ……
    几百毫秒后（取决于磁盘寻道时间），OS 将该数据块放入 page cache
    唤醒挂起的进程并复制给参数 buf 地址处
    返回
endif
```

从上面的伪代码可以看出，当读取的数据在 page cache 中不存在的时候将会挂起操作进程，等待硬件部件读取完成，而这种硬件操作基本都是百毫秒级别。

### 1.2 Write 时发生了什么

```
if page cache 中存在所需的数据块
    复制传入 buf 地址处的数据到 page cache
    返回
else
    向 DMA 控制器发起读磁盘操作
    挂起该操作所在的进程，等待 DMA 操作完成
    ......
    几百毫秒后（取决于磁盘寻道时间），OS 将该数据块放入 page cache
    唤醒挂起的进程并复制传入 buf 地址处的数据到 page cache
    返回
endif
    ......
    内核 flush 线程（采用电梯算法）异步将改动过（脏）的 page cache 写入磁盘
```

如上面的伪代码，让人没想到的是一次`write`竟然会可能产生一次`read`！
       
有了以上 2 个操作的大致过程，有人自然能想到，关于`read`能不能让 OS “预读取”到 page cache？`write`能不能不走 page cache 直接写入磁盘？这些自然已经有前辈们实现出来了，他们就是`readahead(2)`和`open(2)`的`O_DIRECT`参数。

### 1.3 page cache 的特点

page cache 会不会占太多内存？会！当用`free`命令时，Buffer/Cache 部分就是 page cache 和其他各种内核缓存的东西了，随着应用使用的内存越来越多，OS 会逐步释放 page cache 所占用的内存，即 OS 通过 page cache 利用空闲内存，为文件 IO 操作加速，操作同一个文件块的所有进程都能受益！并且是 OS 全局的不随任何进程的生命周期结束而消亡！

page cache 的特点列出如下：

* 自动利用空闲内存
* 自动管理淘汰机制
* 自动加速所有文件操作
* 所有进程受益
* OS 级别全局缓存（即使应用进程崩溃退出）
* 无 GC，几乎不影响应用进程

## 二、何为顺序，何为随机

对上面的 read/write 的简要过程，有两个例子：

* 1GB RAM 的机器冷启动后，1 个 10GB 的磁盘文件，page cache 块大小为 4KB，当每次读取 1Byte 从文件头顺序读到尾的时候，实际只会发生`10*1024*1024/4=2621440`次磁盘读取操作！而假如分别随机读 4000，5000，1048567……位置的数据然后再读取 3200 位置的数据时第 1 个包含 0~4096 数据的 page cache 块可能已经清里掉了，将会再次从磁盘中读取，导致读放大。
 

* 1GB RAM 的机器冷启动后，每次写 1Byte，顺序写 10GB 的数据到磁盘文件，page cache 块大小 4KB，同样也只会发生`10*1024*1024/4=2621440`次磁盘写入操作，如果不是顺序的写入，当再次写之前写过的块时可能会需要再次从磁盘中读取到 page cache。
 
看完以上的例子，可能并没有完全弄清楚问题，到底 OS 内部怎么个顺序/随机？似乎完全看不到。

### 2.1 磁盘 IO 调度与电梯算法

磁盘的物理特性决定，“顺序”意味着磁道的邻近，即磁头移动尽可能少的距离，比如同磁道的“同心圆”，相邻磁道的“同心环”；参考平时乘坐的电梯，为保证最大吞吐和每层用户的平均等待时间电梯都是始终朝一个方向运行到底，然后再向接到请求的相反方向运行；相比之下在磁盘 IO 调度的场景里是磁头从磁盘圆心到边缘，再从边缘到圆心，也就是说 OS 会对请求访问的磁盘块排序，然后使用磁头最小化移动磁头的电梯算法调度磁盘 IO 请求，如下图所示：

<img src="http://cnd.qiniu.lin07ux.cn/markdown/1555599014425.png"/>

这样大概就能理解了磁盘读写的随机与顺序。而且会发现，磁盘和过去 Walkman 听歌的卡带多么相似啊，需要倒带/前进到前后 n 首歌曲，如果随机的一会听第 2，再听 5，再听 1，那就要“人肉卡带播放调度”吧。

> “内存是新的硬盘，硬盘是新的磁带”
> 
> ——图灵奖得主Jim Gray

### 2.2 查询（读）效率 VS 写吞吐量

只要是现实世界就有太多跷跷板的两端让人权（jiu）衡（jie）的东西，计算机领域的 Latency/Throughput、CAP、一致性/读写效率……磁盘 IO 相关的简单来讲就是要“查的快还是要写的快？能不能两者都快？”。可以全都要，但一定会存在缺陷，总之无法完美 ☹ 
来回顾下平时使用的查询和写入数据的场景：

```sql
select xx1, xx2, xx3 from tbl where xx1=|>|<|in …
 
insert into tbl values(9, 5, 7)
insert into tbl values(3, 8, 4)
```

以上的场景很容易发现，当查询的时候需要按“规律”查询，按某字段相等、大于、小于……，而写入的（新）数据其源头来自于真实世界，数据是不可能按照意愿写入，也就是说写入本质上是随机的！但前一节讲过，对磁盘来说随机就意味着慢！效率低！这在 CS 领域是不可接受的。

这里出现的矛盾也就是，将随机的数据顺序写入磁盘很快，但查询（读）怎么办？把数据按查询的“规律”排好序查询很快，但写入前要排序，写又慢了；这似乎是两个不可调和的矛盾，需要权（zhong）衡（yong）之道。

## 三、磁盘 IO 与数据库数据结构

虽然上面提到了数据读写的顺序问题矛盾，但好在 CS 领域汇聚了无数的聪明人，在短短几十年内发明了各种算法、数据结构（包括上面讲的 page cache、disk scheduling），让数据、磁盘之花越开越艳。

### 3.1 花儿为什么这样红之 RDBMS 索引

想到过去经常在面试中提到开放性问题：为什么 Oracle/DB2/MySQL… 这些 niubility 的大厂的世界级 RDBMS 产品的主要索引结构都不约而同的使用 B+ Tree 为基础？它有什么 NB 之处让如此爱重复造轮子的CS领域拿得起放不下？能不能用更平常的 RBTree/AVLTree 代替？其实并不是故意为难，而是对一个数据系统来说这一系列的为什么涉及到磁盘 IO 的方方面面，也即是开头所说的如何利用磁盘的特性做设计的问题。

B+ Tree：

<img src="http://cnd.qiniu.lin07ux.cn/markdown/1555599580273.png"/>

如上图所示，B+ Tree 为有序树，说到有序树自然就能想到 RBTree、AVLTree 这些二叉平衡树，都是有序，为什么 MySQL 不用后者，Java TreeMap 为什么不用 B+ Tree？

上图中树高为 3，如果用 RBTree 树高为 4，比 B+ Tree 高 1 层。RBTree 是二叉树，每个节点只有 1 个数据，如果数据量大，树高将会差距很大，定位一个数据时从根到数据节点，每经过的一个节点有可能就是一次磁盘 IO（page cache 中没有所需的数据块），而且这里每次的磁盘 IO 还是随机的，带来大量的随机磁盘 IO！而 B+ Tree 每个节点（块/页）内为多个有序的数据，大小一般和 page cache 一致（比如 x86 的 4KB）方便装入 page cache，降低了树的高度，也就减少了可能的随机磁盘 IO 操作，而每个块内部的有序数据也利于顺序读取查找；按块划分使部分块有空余空间，方便做 insert，不需要向 RBTree 那样频繁的做 rebalance，要知道 rebalance 在磁盘 IO 上也是效率杀手。

正是在这种软硬件结合的设计思想造就的 B+ Tree，天然适合磁盘这种亲顺序存储的设备；而反观用于内存中的 TreeMap，内存自然没有磁盘的这些限制，随机和顺序操作几乎一样速度，所以更省空间，定位效率更高的 RBTree 被用于这些内存数据结构、容器。 
随着大数据的到来，数据越来越多，《HBase 权威指南》一书里开头部分介绍的大数据来临时 RDBMS 纵向扩展、横向扩展、分库分表、反模式……简直一部血泪史了，其根本原因是随着数据增多 B+ Tree 的树高不断增加，虽然 B+ Tree 的 O(log) 复杂度理论上来说和常数级别差别不大，但每次增高都会因磁盘寻道时间带来的近乎上百毫秒的一次延迟，最终达到不可接受的地步，可见理论和现实实践是有差距的，由此也发展出了新的技术，即后面讲的 LevelDB/HBase 等。

### 3.2 花儿为什么这样红之 “企业级“SSD

WAL、Undo、Redo 这几个词本质上来说都是 Log！是的，Log 在 CS 工程领域是如此的重要，HBase 的 WAL，数据库的 Undo/Redo Log，分布式事务/协调器的 Undo/Redo Log……

这些 Log 和我们平常程序打印输出的 Log 是不同的，这类 Log 是不允许丢失的，当我们write后，相应的数据就必须立刻马上的出现在磁盘的某个扇区上！而不能向之前所讲的停留在 page cache 里然后待内核的 flush 线程异步写到磁盘上，也就是说必须保证“落盘“，因为这类日志本来就是用来故障恢复的，已经是“最后一道防线”了，必须保证立刻的持久化。 
不经过 page cache 保证“落盘”的方法一个是前面讲过的`open(2)`的`O_DIRECT`参数，一个就是`fsync(2)`，后者在程序中使用更多。这些都是高开销操作，家用 PC 上主要是读多写少，而对服务器来说有可能是读写都多，这里的“企业级”SSD 就是说保障读的同时也得保障写的 fsync 效率；就好比消费级旗舰 SSD 是个“偏科生”，而企业级 SSD 才是“真学霸”，当然谁都喜欢“真学霸”，但“真学霸”价钱却不低，所以才有了用“偏科生”冒充“企业级”的，只写 SSD 字眼，大家要擦亮眼睛，搞清楚自己的业务到底是哪种类型。

了解了两者的优缺点，也就对设计/选型更又了把握；当大多数只是读取的时候用消费级 SSD 没问题，比如下载站、web 静态文件……（似乎 CDN 更好😊）；而涉及到大量事务/故障恢复的 DB、分布式一致性算法、需大量保证落盘的写操作……这类就得要仔细的选择到底是哪种 SSD 了。

### 3.3 花儿为什么这样红之 Kafka

> **Don't fear the filesystem!**

这是 Kafaka 文档中第一句标题。

通常认为文件系统/磁盘一定是慢的，宁愿用网络 IO，而 Kafka 的成功告诉说明：一个设计得当的基于磁盘的系统可以和网络一样快！先把 Kafka 的磁盘网络 IO 相关的设计精要列出来：

* 把文件当队列，利用 append 文件的高效顺序磁盘 IO。
* 利用 Zero-Copy 技术减少 User/Kernel Mode 的切换次数和数据复制。
* 利用批量写，提高压缩率，提高网络利用率，减少 write syscall。 
关于第一点，整篇文章都是讲利用磁盘顺序操作的高效率，第二点先看一下当通过网络发送一个文件的过程：

<img src="http://cnd.qiniu.lin07ux.cn/markdown/1555600088927.png"/>

Steps：

1. DMA data from disk to kernel buffer #1
2. copy data from kernel buffer #1 to user buffer
3. copy data from user buffer to kernel buffer #2
4. DMA buffer from kernel buffer #2 to network

> 注：kern buf1 为 page cache，kern buf2 为内核 socket buf。

发送文件这样简单的操作，数据在内存里要复制 3 次，总线上要跑 6 次，如此低效，但实际这可能是时时刻刻都在用的机制。所以有了 zero-copy，比如 linux 的`sendfile`，windows 的`TransmitFile`，其不是标准 POSIX API，故会影响使用这些 API 的应用的移植性。好在 Java 提供了统一的 API`java.nio.channels.FileChannel#transferTo`，这些 zero-copy api 将如上步骤部分的 2-3 步合并为一步，直接将 page cache 的内容复制到 socket buf，减少到 User Mode 的复制，过程如下：

<img src="http://cnd.qiniu.lin07ux.cn/markdown/1555600204412.png"/>

看到这里很多人要问这不是真正的“零“拷贝啊，为什么不直接将 page cache 的内容发送到网卡？当底层硬件网卡支持 gather operations 时，该操作是可以进一步优化为：

<img src="http://cnd.qiniu.lin07ux.cn/markdown/1555600229875.png"/>

最终实现了理想中的真正的“零拷贝“。

Zero-Copy 这么美好，没有缺陷么？有，凡事都有两面性，Zero-Copy 数据不经过 User Mode，所以使用场景只能是将磁盘文件数据不做任何改动的发送到网络，这样的场景很多，比如 web 的静态资源、下载站、CDN……这个限制直接对 Kafka 的设计造成一定影响：

* 只支持 End-to-End 的压缩
* 因上一条，所有 Consumer 必须支持 Producer 采用的压缩格式
* broker 和 Consumer 间不能对数据做任何转码/过滤/压缩/解压的操作

Kafka 可谓利用硬件和现代 OS 特性而设计的典范，基于这些设计特点，针对 Kafka 集群的机器需要有如下特点：

* 高转速磁盘，无需 RAID，支持快速顺序 append 操作。
* “企业量级”的 fsync 速度和保障。
* 网卡以及驱动要支持 gather operations。
* 内存大小要支持绝大多数消费者 offset 到生产者写入这段数据的 page cache 支持。

### 3.4 花儿为什么这样红之 LevelDB、HBase

将 LevelDB 与 HBase 放在一起，是因为这 2 个东西底层存储查询机制几乎一模一样，HBase 几乎就是 LevelDB 的分布式翻版产品。
       
一直在讲“顺序快，随机慢“，但目标是读写都要顺序都要快！前面知道 B+ Tree 在数据量过大的情况下已经跪了，现在讲的这 2 个东西就是为了解决这种大数据量的场景。

可能已经有人想到 B+ Tree 在大数据量时跪了是因为树高太高了，如果拆分成多个树不就解决了么？对，LevelDB/HBase 本质上来说就是这种思想，只不过把树的拆分/合并全部自动化了，并且起了个新名字叫 LSM Tree：

<img src="http://cnd.qiniu.lin07ux.cn/markdown/1555600421414.png"/>

上图主要分为 2 部分，In Memory 和 On Disk，不管是在内存还是磁盘中的数据结构都是有序的，新增改的数据在内存中采用 RBTree/SkipList 等构建有序数据结构，在磁盘上保存有序的静态文件，通过一次次的合并（compact）将小的有序数据文件合并为大的有序数据文件，正如我们一直反复说的，一切都是为了磁盘操作的有序。合并的过程其实就是 merge sort 外排序的过程：

<img src="http://cnd.qiniu.lin07ux.cn/markdown/1555600469897.png"/>

In Memory 数据的有序性在需要写到磁盘上时利于批量写的同时可直接按顺序写出，顺序的数据又利于查询（读），简直是两全其美啊，然而任何设计都有其两面性，很难做到“我全都要”：

* 曾经有人问 HBase 能不能不 compact？答案显然是不行的，并且 compact 越来越成为 HBase 等大数据产品的痛点，因为 compact 是高 IO 操作，如果业务有峰谷还好，可以控制在低谷时段人工触发 compact，反之将会对 TPS/QPS 产生影响。


* 关于 HBase 还有个常见问题，读取老的冷数据为什么会引起 TPS/QPS 的剧烈抖动？真正的答案并不是线程、CPU 资源抢占，而是我们一开头讲的 page cache，HBase 读写的应该都是尾部的热数据，如果大量读取冷数据，冷数据加载到 page cache 后必然将热数据挤出去，而热数据又是主要业务操作的数据，必然造成抖动。 
细想一下 LSM Tree 的设计也是一种空间换时间的策略，利用内存中方便排序的特点保存一批有序的数据，然后有序顺序的写入磁盘文件，然后再通过外排序算法——merge sort，将多个小的有序文件合并为大的有序文件……似乎每句话都离不开“序”这个字。是的，当做应用时用 qsort、std::sort、Collections.sort()、order by……很少有去在意具体的某个排序算法，但当探入底层结构时才发现排序算法几乎是这些数据产品得以实现的基石。

> “勿在浮沙筑高台”
> 
> ——《深入浅出MFC》侯捷著


