> 转摘：[支撑百万并发的 “零拷贝” 技术，你了解吗？](https://mp.weixin.qq.com/s/xej6klx2q0G1fp82_vKCOg)
> 
> 由于 IO 操作都需要利用内存作为中转，所以需要先了解 Linux 中虚拟内存和内存管理相关的知识，参考 [Linux 内存管理](./Linux%20内存管理)。

零拷贝(Zero-copy)技术指在计算机执行操作时，CPU 不需要先将数据从一个内存区域复制到另一个内存区域，从而减少上下文切换以及 CPU 的拷贝时间。其主要作用为：

* 减少数据在内核缓冲区和用户进程缓冲区之间反复的 I/O 拷贝操作。
* 减少用户进程地址空间和内核地址空间之间因为上下文切换而带来的 CPU 开销。

在了解 Linux 的零拷贝前，先简单地了解一下相关的概念：

* **上下文切换**：当用户程序向内核发起系统调用时，CPU 将用户进程从用户态切换到内核态；当系统调用返回时，CPU 将用户进程从内核态切换回用户态。
* **CPU 拷贝**：由 CPU 直接处理数据的传送，数据拷贝时会一直占用 CPU 的资源。
* **DMA**：Direct Memory Access，由 CPU 向 DMA 磁盘控制器下达指令，让 DMA 控制器来处理数据的传送，数据传送完毕再把信息反馈给 CPU，从而减轻了 CPU 资源的占有率。
* **MMU**：Memory Management Unit，内存管理单元，主要实现竞争访问保护管理需求、高效的翻译转换管理需求、高效的虚实内存交换需求。
* **Page Cache**：为了避免每次读写文件时，都需要对硬盘进行读写操作，Linux 内核使用也缓存机制来对文件中的数据进行缓存。为了更好的利用程序数据的空间局部性，Page Cache 使用了预读功能。

> 零拷贝主要希望达到的目的是*零 CPU 拷贝*，避免占用 CPU 资源。

## 一、Linux I/O 读写方式

Linux 提供了轮序、I/O 中断及 DMA 传输这 3 种磁盘与主存之间的数据传输机制。其中：

* 轮询方式是基于死循环对 I/O 端口进行不断检测。
* I/O 中断方式是指当数据到达时，磁盘主动向 CPU 发起中断请求，由 CPU 自身负责数据的传输过程。
* DMA 传输则在 I/O 中断的基础上引入了 DMA 磁盘控制器，由 DMA 磁盘控制器负责数据的传输，降低了 I/O 中断操作对 CPU 资源的大量消耗。

轮询方式较为简单，下面主要介绍 I/O 中断和 DMA 传输原理。

### 1.2 I/O 中断原理

在 DMA 技术出现之前，应用程序与磁盘之间的 I/O 操作都是通过 CPU 的中断完成的。每次用户进程读取磁盘数据时，都需要 CPU 中断，然后发起 I/O 请求等待数据读取和拷贝完成，**每次的 I/O 中断都导致 CPU 的上下文切换**。

流程图如下：

![](http://cnd.qiniu.lin07ux.cn/markdown/1571033930210.png)

描述如下：

1. 用户进程向 CPU 发起 read 系统调用，读取数据，由用户态切换为内核态，然后一直阻塞等待数据的返回。
2. CPU 在接收到指令以后对磁盘泛起 I/O 请求，将磁盘数据先放入到磁盘控制器缓冲区。
3. 数据在磁盘控制器缓存区准备完成以后，磁盘向 CPU 发起 I/O 中断。
4. CPU 收到 I/O 中断以后将磁盘缓冲区中的数据拷贝到内核缓冲区，然后再从内核缓冲区拷贝到用户缓冲区。
5. 用户进程由内核态切换回用户态，解除阻塞状态，然后等待 CPU 的下一个执行时钟。

### 1.3 DMA 传输原理

DMA 的全称叫直接内存存取(Direct Memory Access)，是一种允许外围设备（硬件子系统）直接访问系统主内存的机制。

也就是说，基于 DMA 访问方式，系统主内存于硬盘或网卡之间的数据传输可以绕开 CPU 的全程调度。

目前大多数的硬件设备，包括磁盘控制器、网卡、显卡以及声卡等都支持 DMA 技术。

![](http://cnd.qiniu.lin07ux.cn/markdown/1571034189512.png)

整个数据传输操作在一个 DMA 控制器的控制下进行的。CPU 除了在数据传输开始和结束时做一点处理外（开始和结束时候要做中断处理），在传输过程中 CPU 可以继续进行其他的工作。这样在大部分时间里，CPU 计算和 I/O 操作都处于并行操作，使整个计算机系统的效率大大提高。

有了 DMA 磁盘控制器接管数据读写请求以后，CPU 从繁重的 I/O 操作中解脱，数据读取操作的流程流程如下：

![](http://cnd.qiniu.lin07ux.cn/markdown/1571034245486.png)

描述如下：

1. 用户进程向 CPU 发起 read 系统调用读取数据，由用户态切换为内核态，然后一直阻塞等待数据的返回。
2. CPU 在接受到指令以后，对 DMA 磁盘控制器发起调度指令。
3. DMA 磁盘控制器对磁盘发起 I/O 请求，将次哦按数据先放入磁盘控制器缓冲区，CPU 全程不参与此过程。
4. 数据读取完成后，DMA 磁盘控制器会接收到磁盘的通知，将数据从磁盘控制器缓冲区拷贝到内核缓冲区。
5. DMA 磁盘控制器向 CPU 发出数据读取完的信号，由 CPU 负责将数据从内核缓冲区拷贝到用户缓冲区。
6. 用户进程由内核态切换回用户态，解除阻塞状态，然后等待 CPU 的下一个执行时钟。

## 二、传统 I/O 方式

为了更好的理解零拷贝解决的问题，首先了解一下传统 I/O 方式存在的问题。

在 Linux 系统中，传统的访问方式是通过`write()`和`read()`两个系统调用实现的：通过`read()`函数读取文件到到缓存区中，然后通过`write()`方法把缓存中的数据输出到网络端口。

伪代码如下：

```c
read(file_fd, tmp_buf, len);
write(socket_fd, tmp_buf, len);
```

下图分别对应传统 I/O 操作的数据读写流程，整个过程涉及 2 次 CPU 拷贝、2 次 DMA 拷贝(总共 4 次拷贝)，以及 4 次上下文切换：

![](http://cnd.qiniu.lin07ux.cn/markdown/1571034573921.png)

### 2.1 传统读操作

当应用程序执行 read 系统调用读取一块数据的时候，如果这块数据已经存在于用户进程的页内存中，就直接从内存中读取数据。如果数据不存在，则先将数据从磁盘加载数据到内核空间的读缓存(read buffer)中，再从读缓存拷贝到用户进程的页内存中。

伪代码如下：

```c
read(file_fd, tmp_buf, len);
```

*基于传统的 I/O 读取方式，`read()`系统调用会触发 2 次上下文切换，1 次 DMA 拷贝和 1 次 CPU 拷贝*。 
发起数据读取的流程如下：

1. 用户进程通过`read()`函数向内核发起系统调用，上下文从用户态切换为内核态。
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间的读缓冲区(read buffer)。
3. CPU 将读缓冲区(read buffer)中的数据拷贝到用户空间的用户缓冲区(user buffer)。
4. 上下文从内核态切换回用户态，read 调用执行返回。

### 2.2 传统写操作

当应用程序准备好数据，执行 write 系统调用发送网络数据时，先将数据从用户空间的页缓存拷贝到内核空间的网络缓冲区(socket buffer)中，然后再将写缓存中的数据拷贝到网卡设备完成数据发送。

伪代码如下：

```c
write(socket_fd, tmp_buf, len);
```

*基于传统的 I/O 写入方式，`write()`系统调用会触发 2 次上下文切换，1 次 CPU 拷贝和 1 次 DMA 拷贝。*

用户程序发送网络数据的流程如下：

1. 用户进程通过`write()`函数向内核发起系统调用，上下文从用户态切换为内核态。
2. CPU 将用户缓冲区中的数据拷贝到内核空间的网络缓冲区。
3. CPU 利用 DMA 控制器将数据从网络缓冲区拷贝到网卡进行数据传输。
4. 上下文从内核态切换回用户态，`write()`系统调用执行返回。

## 三、零拷贝 I/O 方式

在 Linux 中零拷贝技术主要有 3 个实现思路：

1. **用户态直接 I/O**：应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，硬件上的数据直接拷贝至了用户空间，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。

2. **减少数据拷贝次数**：在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的 CPU 拷贝，以及数据在系统内核空间内的 CPU 拷贝，这也是当前主流零拷贝技术的实现思路。

3. **写时复制技术**：写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么将其拷贝到自己的进程地址空间中，如果只是数据读取操作则不需要进行拷贝操作。 
### 3.1 用户态直接 I/O

用户态直接 I/O 使得应用进程或运行在用户态（user space）下的库函数直接访问硬件设备。 
数据直接跨过内核进行传输，内核在数据传输过程除了进行必要的虚拟存储配置工作之外，不参与任何其他工作，这种方式能够直接绕过内核，极大提高了性能。

![](http://cnd.qiniu.lin07ux.cn/markdown/1571037569094.png)

用户态直接 I/O 只能适用于不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。

其次，这种零拷贝机制会直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成大量资源的浪费，解决方案是配合异步 I/O 使用。

### 3.2 mmap + write

使用`mmap + write`代替原来的`read + write`方式，可以减少 1 次 CPU 拷贝操作(减少了内核缓冲区和用户缓冲区的拷贝，但增加了一次内核读缓冲区到输出设备缓冲区的拷贝)。 
`mmap`是 Linux 提供的一种内存映射文件方法，即将一个进程的地址空间中的一段虚拟地址映射到磁盘文件地址，`mmap + write`的伪代码如下：

```c
tmp_buf = mmap(file_fd, len);
write(socket_fd, tmp_buf, len);
``` 
使用 mmap 的目的是将内核中读缓冲区(read buffer)的地址与用户空间的缓冲区(user buffer)进行映射，从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区拷贝到用户缓冲区的过程。然而内核读缓冲区仍需将数据拷贝到内核写缓冲区。

大致的流程如下图所示：

![](http://cnd.qiniu.lin07ux.cn/markdown/1571037827172.png)

*基于`mmap + write`系统调用的零拷贝方式，整个拷贝过程会发生 4 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝。*

用户程序读写数据的流程如下：

1. 用户进程通过`mmap()`函数向内核发起系统调用，上下文从用户态切换为内核态。
2. 将用户进程的内核空间的读缓冲区用用户空间的缓冲区进行内存地址映射。
3. CPU 利用 DMA 控制器将数据从主从或硬盘拷贝到内核空间的读缓冲区。
4. 上下文从内核态切换回用户态，`mmap()`系统调用执行返回。
5. 用户进程通过`write()`函数向内核发起系统调用，上下文从用户态切换为内核态。
6. CPU 将内核读缓冲区中的数据拷贝到网络缓冲区(Socket Buffer)。
7. CPU 利用 DMA 控制器将数据从网络缓冲区拷贝到网卡进行数据传输。
8. 上下文从内核态切换回用户态，`write()`系统调用执行返回。

mmap 主要的用处是提高 I/O 性能，特别是针对大文件。对于小文件，内存映射文件反而会导致碎片空间的浪费。因为内存映射总是要对齐页边界，最小单位是 4 KB，一个 5 KB 的文件将会映射占用 8 KB 内存，也就会浪费 3 KB 内存。

mmap 的拷贝虽然减少了 1 次拷贝，提升了效率，但也存在一些隐藏的问题。当 mmap 一个文件时，如果这个文件被另一个进程所截获，那么`write()`系统调用会因为访问非法地址被`SIGBUS`信号终止，`SIGBUS`默认会杀死进程并产生一个`coredump`，服务器可能因此被终止。

### 3.3 Sendfile

Sendfile 系统调用在 Linux 内核版本 2.1 中被引入，目的是简化通过网络在两个通道之间进行的数据传输过程。

通过 Sendfile 系统调用，数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。Sendfile 系统调用的引入，不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数。

与 mmap 内存映射方式不同的是， Sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。

伪代码如下：

```c
sendfile(socket_fd, file_fd, len);
```

流程图如下所示：

![](http://cnd.qiniu.lin07ux.cn/markdown/1571038387504.png)

*基于 Sendfile 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝。*

> Sendfile 相当于将读和写变成了一个操作，所以只需要两次上下文切换。

用户程序读写数据的流程如下：

1. 用户进程通过`sendfile()`函数向内核发起系统调用，上下文从用户态切换为内核态。
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间的读缓冲区。
3. CPU 将读缓冲区中的数据拷贝到网络缓冲区。
4. CPU 利用 DMA 控制器将数据从网络缓冲区拷贝到网卡进行数据传输。
5. 上下文从内核态切换回用户态，Sendfile 系统调用执行返回。

相比较于 mmap 内存映射的方式，Sendfile 少了 2 次上下文切换，但是仍然有 1 次 CPU 拷贝操作。

Sendfile 存在的问题是用户程序不能对数据进行修改，而只是单纯地完成了一次数据传输过程。

### 3.4 Sendfile + DMA gather copy

Linux 2.4 版本的内核对 Sendfile 系统调用进行修改，为 DMA 拷贝引入了 gather 操作。

DMA gather copy 可以将内核空间的读缓冲区中对应的数据描述信息（内存地址、地址偏移量）记录到相应的网络缓冲区中，由 DMA 根据内存地址、地址偏移量将数据批量地从内核读缓冲区直接拷贝到网卡设备中。这样就省去了内核空间中仅剩的 1 次 CPU 拷贝操作。

在硬件的支持下，Sendfile 拷贝方式不再从内核缓冲区的数据拷贝到 socket 缓冲区，取而代之的仅仅是缓冲区文件描述符和数据长度的拷贝。这样 DMA 引擎直接利用 gather 操作将页缓存中数据打包发送到网络中即可，本质就是和虚拟内存映射的思路类似。

伪代码如下：

```c
sendfile(socket_fd, file_fd, len);
```

流程图如下：

![](http://cnd.qiniu.lin07ux.cn/markdown/1571038676661.png)

*基于`Sendfile + DMA gather copy`系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换、0 次 CPU 拷贝以及 2 次 DMA 拷贝。*

用户程序读写数据的流程如下：

1. 用户进程通过`sendfile()`函数向内核发起系统调用，上下文从用户态切换为内核态。
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间的读缓冲区。
3. CPU 把读缓冲区的文件描述符和数据长度拷贝到网络缓冲区。
4. 基于已拷贝的文件描述符和数据长度，CPU 利用 DMA 控制器的 gather/scatter 操作直接批量的将数据从内核的读缓冲区拷贝到网卡进行数据传输。
5. 上下文从内核态切换回用户态，Sendfile 系统调用执行返回。

Sendfile + DMA gather copy 拷贝方式与 Sendfile 一样，存在用户程序不能对数据进行修改的问题，而且本身需要硬件的支持，它只适用于将数据从文件拷贝到 socket 套接字上的传输过程。

### 3.5 Splice

Sendfile 只适用于将数据从文件拷贝到 socket 套接字上，同时需要硬件的支持，这也限定了它的使用范围。

Linux 在 2.6.17 版本引入 Splice 系统调用，不仅不需要硬件支持，还实现了两个文件描述符之间的数据零拷贝。

Splice 系统调用可以在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline），从而避免了两者之间的 CPU 拷贝操作。

伪代码如下：

```c
splice(fd_in, off_in, fd_out, off_out, len, flags);
```

流程图如下：

![](http://cnd.qiniu.lin07ux.cn/markdown/1571038914051.png)

*基于 Splice 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，0 次 CPU 拷贝以及 2 次 DMA 拷贝。*

用户程序读写数据的流程如下：

1. 用户进程通过`splice()`函数向内核发起系统调用，上下文从用户态切换为内核态。
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间的读缓冲区。
3. CPU 在内核空间的读缓冲区和网络缓冲区之间建立管道。
4. CPU 利用 DMA 控制器将数据从网络缓冲区拷贝到网卡进行数据传输。
5. 上下文从内核态切换回用户态，Splice 系统调用执行返回。

Splice 拷贝方式也同样存在用户程序不能对数据进行修改的问题。除此之外，它使用了 Linux 的管道缓冲机制，可以用于任意两个文件描述符中传输数据，但是它的两个文件描述符参数中有一个必须是管道设备。

### 3.6 写时复制

在某些情况下，内核缓冲区可能被多个进程所共享，如果某个进程想要这个共享区进行 write 操作，由于 write 不提供任何的锁操作，那么就会对共享区中的数据造成破坏，写时复制的引入就是 Linux 用来保护数据的。

写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么就需要将其拷贝到自己的进程地址空间中。

这样做并不影响其他进程对这块数据的操作，每个进程要修改的时候才会进行拷贝，所以叫写时拷贝。

这种方法在某种程度上能够降低系统开销，如果某个进程永远不会对所访问的数据进行更改，那么也就永远不需要拷贝。

### 3.7 缓冲区共享

缓冲区共享方式完全改写了传统的 I/O 操作，因为传统 I/O 接口都是基于数据拷贝进行的，要避免拷贝就得去掉原先的那套接口并重新改写。所以这种方法是比较全面的零拷贝技术，目前比较成熟的一个方案是在 Solaris 上实现的`fbuf`(Fast Buffer，快速缓冲区)。

`fbuf`的思想是每个进程都维护着一个缓冲区池，这个缓冲区池能被同时映射到用户空间和内核空间，内核和用户共享这个缓冲区池，这样就避免了一系列的拷贝操作。

流程图如下：

![](http://cnd.qiniu.lin07ux.cn/markdown/1571039160838.png)

缓冲区共享的难度在于管理共享缓冲区池需要应用程序、网络软件以及设备驱动程序之间的紧密合作，而且如何改写 API 目前还处于试验阶段，并不成熟。

## 四、其他

### 4.1 零拷贝方式对比

无论是传统 I/O 拷贝方式还是引入零拷贝的方式，2 次 DMA Copy 是都少不了的，因为两次 DMA 都是依赖硬件完成的。

下面从 CPU 拷贝次数、DMA 拷贝次数以及系统调用几个方面总结一下上述几种 I/O 拷贝方式的差别：

  拷贝方式                   |  CPU 拷贝  |  DMA 拷贝  |  系统调用   |  上下文切换
----------------------------|-----------|------------|-----------|-------------
 read + write(传统方式)      |   2       |   2        | read/write |  4
 mmap + write(内存映射)      |   1       |   2        | mmap/write |  4
 sendfile                   |   1       |   2        | sendfile   |  2
 sendfile + DMA gather copy |   0       |   2        | sendfile   |  2
 splice                     |   0       |   2        | splice     |  2

### 4.2 RocketMQ 和 Kafka 零拷贝对比

RocketMQ 选择了`mmap + write`这种零拷贝方式，适用于业务级消息这种小块文件的数据持久化和传输。

Kafka 采用的是 Sendfile 这种零拷贝方式，适用于系统日志消息这种高吞吐量的大块文件的数据持久化和传输。

> 值得注意的一点是，Kafka 的索引文件使用的是`mmap + write`方式，数据文件使用的是 Sendfile 方式。

 消息队列  | 零拷贝方式     | 优点                               | 缺点
----------|--------------|-----------------------------------|----------------------------------
 RocketMQ | mmap + write | 适用于小块文件传输，频繁调用时，效率很高 | 不能很好的利用 DMA 方式，会比 sendfile 多消耗 CPU，内存安全性控制复杂，需要避免 JVM Crash 问题
 Kafka    | sendfile     | 可以利用 DMA 方式，消耗 CPU 较少，大块文件传输效率高，无内存安全性问题 | 小块文件效率低于 mmap 方式，只能是 BIO 方式传输，不能使用 NIO 方式

### 4.3 大文件传输
 
 在大文件传输的场景下，零拷贝技术并不是最优选择，因为在零拷贝的任何一种实现中，都会有 DMA 将数据从磁盘拷贝到内核缓冲区——Page Cache 这一步。而在传输大文件（GB 级别的文件）的时候，因为大文件会占用过多的 Page Cache 空间，挤占小文件的读写 Cache 空间，使得 Page Cache 无法达到预期的作用，反而会浪费一次 DMA 做的一次数据拷贝，造成性能降低。

在大文件传输场景时，可以采用异步 I/O + Direct I/O 方式来提升性能。Direct I/O 绕过了 Page Cache，所以无法享受内核的优化：内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后合并成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作；内核也会预读后续的 I/O 请求放在 Page Cache 中，一样是为了减少对磁盘的操作。

在 Nginx 的实际应用中也会考虑大文件传输对 Page Cache 的影响，可以用如下的配置来根据文件的大小来使用不同的方式传输：

```conf
location /video/ {
    sendfile on;
    aio on;
    directio 1024m;
}
```

这段配置会在文件大小大于 directio 值后，使用异步 I/O + Direct I/O 的方式传输文件，否则就使用 sendfile 零拷贝方式传输文件。

不过，Direct I/O 也有一些缺点，如：强制地址对齐限制、平台不兼容、不能并发地运行 fork 和 Direct I/O 等。

### 4.4 内核缓冲区和用户缓冲区之间的传输优化

前面讨论的各种 zero-copy 技术都是基于减少甚至避免用户空间和内核空间之间的 CPU 数据拷贝的。虽然有些技术非常高效，但大多都有适用性很窄的问题。比如 sendfile 和 splice 很高效，但是都适用于哪些用户进程不需要再处理的场景，比如静态文件服务器或者直接转发数据的代理服务器。

如果要实现用户进程内处理数据（这种场景比直接转发数据更加常见）之后在发送出去的话，用户空间和内核空间的数据传输就是不可避免的。

有两种优化用户空间和内核空间数据传输的技术：

* 动态重映射和写时拷贝 Copy-on-Write

    Copy-on-Write 简称 COW，是一种建立在虚拟内存重映射技术之上的技术，因此它需要 MMU 的硬件支持。COW 这种零拷贝技术比较适用于*多读少写*的场景，这可以使得 COW 事件发生较少，而在其他场景下反而可能造成负优化，因为 COW 事件所带来的的系统开销要远远高于一次 CPU 拷贝锁产生的开销。
    
    COW 的实际应用比较有代表性的是 Redis 的持久化机制：Redis 如果采用 bgsave 或者 bgrewriteaof 命令进行备份，会 fork 一个子进行来将数据存到磁盘中。在备份的过程中会采用 COW 机制来尽量避免内存数据的拷贝和数据冲突。

* 缓冲区共享 Buffer Sharing：

    传统的 Linux I/O 接口都是基于复制/拷贝的，为实现这种模式的 I/O，Linux 必须要在每一个 I/O 操作时进行虚拟内存映射和解除。其效率严重受限于缓存体系结构、MMU 地址转换速度和 TLB 命中率。
    
    Linux 内核提供了一种叫 fbufs(Fast Buffers)的缓冲区共享的框架，能够避免处理 I/O 请求的虚拟地址转换和 TLB 刷新所带来的的开销，从而极大的提升 I/O 性能。
    
    不过共享缓冲区技术的实现需要依赖用户进程、操作系统内核、I/O 子系统（设备驱动程序、文件系统等）之间协同工作。所以使用过程中常会出现问题且难以 debug。因此这一类的技术还没有成熟和广泛应用，大多处于实验阶段。


